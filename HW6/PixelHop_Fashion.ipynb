{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PixelHop_Fashion.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"code","metadata":{"id":"rn-iSUxsNBzL"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\", force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OIRKTK2YNEyO"},"source":["%ls /content/gdrive/My\\ Drive/Colab\\ Notebooks/Digital\\ Image\\ Processiing/HW6/*.py\n","import sys\n","sys.path.append('/content/gdrive/My Drive/Colab Notebooks/Digital Image Processiing/HW6')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nYauPoA-NOVw"},"source":["# import datasets\n","from keras.datasets import fashion_mnist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VKtkhspNT2K"},"source":["# import base python libraries\n","import pandas as pd\n","import numpy as np\n","from numpy import save\n","from numpy import load\n","import matplotlib.pyplot as plt\n","import warnings, gc\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qGlKbfY-NT5T"},"source":["# import ml framework libraries\n","import keras\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, Activation\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.utils import np_utils\n","from keras import backend as K\n","import xgboost as xgb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"24RcXo_CNT_q"},"source":["# import sklearn libs\n","from sklearn import datasets\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from skimage.measure import block_reduce\n","from skimage.util import view_as_windows\n","from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n","import pickle\n","#from xgboost import XGBClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qIlu2J0XNZoh"},"source":["# special imports\n","from pixelhop import Pixelhop"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4ymZM_nNZrj"},"source":["np.random.seed(1)\n","\n","# Preprocess\n","N_Train_Reduced = 10000    # 10000\n","N_Train_Full = 60000       # 50000\n","N_Test = 10000             # 10000\n","\n","BS = 2000 # batch size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lA1YxpZTNZuq"},"source":["def shuffle_data(X, y):\n","    shuffle_idx = np.random.permutation(y.size)\n","    X = X[shuffle_idx]\n","    y = y[shuffle_idx]\n","    return X, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ueRapZeCNkt5"},"source":["def select_balanced_subset(images, labels, use_num_images):\n","    '''\n","    select equal number of images from each classes\n","    '''\n","    num_total, H, W, C = images.shape\n","    num_class = np.unique(labels).size\n","    num_per_class = int(use_num_images / num_class)\n","\n","    # Shuffle\n","    images, labels = shuffle_data(images, labels)\n","\n","    selected_images = np.zeros((use_num_images, H, W, C))\n","    selected_labels = np.zeros(use_num_images)\n","\n","    for i in range(num_class):\n","        selected_images[i * num_per_class:(i + 1) * num_per_class] = images[labels == i][:num_per_class]\n","        selected_labels[i * num_per_class:(i + 1) * num_per_class] = np.ones((num_per_class)) * i\n","\n","    # Shuffle again\n","    selected_images, selected_labels = shuffle_data(selected_images, selected_labels)\n","    print(selected_labels)\n","\n","    return selected_images, selected_labels\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhURy8SQNkxd"},"source":["def Shrink(X, shrinkArg):\n","    #---- max pooling----\n","    pool = shrinkArg['pool']\n","\n","    ch = X.shape[-1]  # get the number of channels\n","\n","     # fill in the rest of max pooling\n","    if (pool > 1):\n","      X = block_reduce(X, block_size = (1, pool , pool, ch), func = np.max)\n","\n","    #---- neighborhood construction\n","    win = shrinkArg['win']\n","    stride = shrinkArg['stride']\n","    pad = shrinkArg['pad']\n","    \n","    # fill in the rest of neighborhood construction\n","   \n","    if pad > 0:\n","      X = np.pad(X, ((0,0), (pad, pad), (pad, pad), (0, 0)), 'reflect')\n","    X = view_as_windows(X, (1, win , win , 1 ), (1, stride, stride, 1))\n","    X = X.reshape(X.shape[0], X.shape[1], X.shape[2], -1)\n","\n","    print(X.shape[0], X.shape[1], X.shape[2], -1)\n","    return  X\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mpHS8S0NNk1D"},"source":["# example callback function for how to concate features from different hops\n","def Concat(X, concatArg):\n","    return X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AEE5rZS-NrIx"},"source":["def get_feat(p2, X, num_layers=3):\n","    output = p2.transform_singleHop(X,layer=0)\n","    if num_layers>1:\n","        for i in range(num_layers-1):\n","            output = p2.transform_singleHop(output, layer=i+1)\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pOLoJOV-NrL8"},"source":["warnings.filterwarnings(\"ignore\")\n","# ---------- Load MNIST data and split ----------\n","(x_train, y_train), (x_test,y_test) = fashion_mnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MwgQ1te5NrPQ"},"source":["# -----------Data Preprocessing-----------\n","x_train = np.asarray(x_train,dtype='float32')[:,:,:,np.newaxis]\n","x_test = np.asarray(x_test,dtype='float32')[:,:,:,np.newaxis]\n","y_train = np.asarray(y_train,dtype='int')\n","y_test = np.asarray(y_test,dtype='int')\n","\n","# if use only 10000 images train pixelhop\n","x_train_reduced, y_train_reduced = select_balanced_subset(x_train, y_train, use_num_images=N_Train_Reduced)\n","\n","x_train /= 255.0\n","x_test /= 255.0\n","\n","x_train_reduced /=255.0\n","#x_test /=255.0\n","\n","print(\" input feature full train dataset shape: %s\"%str(x_train.shape))\n","print(\" input feature full test dataset shape: %s\"%str(x_test.shape))\n","\n","print(\" input feature reduced train dataset shape: %s\"%str(x_train_reduced.shape))\n","print(\" input feature reduced test dataset shape: %s\"%str(y_train_reduced.shape))\n","print(\" y_train shape: %s\"%str(y_train_reduced))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KdUhN4vaNwjX"},"source":["# -----------Module 1: set PixelHop parameters-----------\n","module1_start_time = time.time()\n","\n","# fill in this part\n","shrinkArgs = [{'func': Shrink, 'win':5, 'stride': 1, 'pad':2, 'pool': 1},\n","              {'func': Shrink, 'win':5, 'stride': 1, 'pad':0, 'pool': 2 }, \n","              {'func': Shrink, 'win':5, 'stride': 1, 'pad':0, 'pool': 2}]\n","          \n","          \n","SaabArgs = [{'num_AC_kernels':-1, 'needBias':False, 'cw': False},\n","            {'num_AC_kernels':-1, 'needBias':True,  'cw': False}, \n","            {'num_AC_kernels':-1, 'needBias':True,  'cw': False}]\n","        \n","\n","concatArg = {'func':Concat}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XOlyFe6pNwmg"},"source":["# -----------Module 1: Train PixelHop -----------\n","# TODO: fill in this part\n","pixelplus = Pixelhop(depth = 3,\n","                    TH1 = 0.0005,  #will vary this part\n","                    TH2 = 0.001,\n","                    SaabArgs = SaabArgs,\n","                    shrinkArgs = shrinkArgs, \n","                    concatArg = concatArg).fit(x_train_reduced)\n","\n","print(\"Module 1 took {0} seconds.\".format(time.time() - module1_start_time))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T0E8Zv-dyK3N"},"source":["# Get K1 , K2, K3\n","Hop1_Energy = pixelplus.Energy['Layer0']  # list of length = K1\n","Hop2_Energy = pixelplus.Energy['Layer1']  # list of length = K2\n","Hop3_Energy = pixelplus.Energy['Layer2']  # list of length  = K3\n","\n","print(\"Hop1 Energy Shape :\", Hop1_Energy[0].shape)\n","print(\"Hop2 Energy Shape :\",Hop1_Energy[0].shape)\n","print(\"Hop3 Energy Shape :\",Hop1_Energy[0].shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5BpXGWXmNwxD"},"source":["# save  the Pixelhop model                    \n","#pickle.dump(pixelplus, open('/content/gdrive/My Drive/Colab Notebooks/Digital Image Processiing/HW6/pixelHop_4units.sav', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O6e91U_kN50z"},"source":["# --------- Module 2: get only Hop 3 feature for both training set and testing set -----------\n","# you can get feature \"batch wise\" and concatenate them if your memory is restricted\n","# TODO: fill in this part\n","#train_hop1_feats = pixelplus.transform(x_train)\n","#train_hop3_feats = get_feat(pixelplus, x_train_reduced)\n","#test_hop3_feats = get_feat(pixelplus, x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UbN_iD3NcxZg"},"source":["# --------- Module 2: get only Hop 3 feature for both training set and testing set -----------\n","# you can get feature \"batch wise\" and concatenate them if your memory is restricted\n","# TODO: fill in this part\n","\n","module2_start_time = time.time()\n","\n","# apply transform to whole 60k train images in batches\n","op1 = get_feat(pixelplus, x_train[:10000,:,:])\n","op2 = get_feat(pixelplus, x_train[10000:20000,:,:])\n","op3 = get_feat(pixelplus, x_train[20000:30000,:,:])\n","op4 = get_feat(pixelplus, x_train[30000:40000,:,:])\n","op5 = get_feat(pixelplus, x_train[40000:50000,:,:])\n","op6 = get_feat(pixelplus, x_train[50000:60000,:,:])\n","\n","# apply transform to 10k test images\n","test_hop3_feats = get_feat(pixelplus, x_test)\n","\n","# get only hop3 features by concatenating the batched transforms\n","#train_hop3_feats = np.concatenate((op1[3],op2[3],op3[3],op4[3],op5[3], op6[3]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JMy7xxc-fRx1"},"source":["# get only hop3 features by concatenating the batched transforms\n","train_hop3_feats = np.concatenate((op1,op2,op3,op4,op5,op6))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r5mHyFICN534"},"source":["print( \"Hop3 train features shape: \", train_hop3_feats.shape)\n","print(\"Hop3 test features shape: \", test_hop3_feats.shape) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8KrviZXhN560"},"source":["# --------- Module 2: standardization of the Hop features\n","STD = np.std(train_hop3_feats, axis=0, keepdims=1)\n","train_hop3_feats = train_hop3_feats/STD\n","test_hop3_feats = test_hop3_feats/STD\n","\n","print(\"Standardized vector shape: \", STD.shape) \n","print( \"Hop3 train features shape: \", train_hop3_feats.shape)\n","print(\"Hop3 test features shape: \", test_hop3_feats.shape) \n","print(\"input feature train shape: %s\"%str(y_train.shape)) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rnq5hCdKOBsv"},"source":["train_hop3_feats = np.squeeze(train_hop3_feats)\n","test_hop3_feats = np.squeeze(test_hop3_feats)\n","\n","print( \"Hop3 train features shape: \", train_hop3_feats.shape)\n","print(\"Hop3 test features shape: \", test_hop3_feats.shape) \n","\n","print(\"Module 2 took {0} seconds.\".format(time.time() - module2_start_time))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yK8f6f3kOBvs"},"source":[" #---------- Module 3: Train XGBoost classifier on hop3 feature ---------\n","\n","module3_start_time = time.time()\n","\n","tr_acc = []\n","te_acc = []\n","\n","# multiclass classification, so use multi:softprob\n","clf = xgb.XGBClassifier(n_jobs=-1,\n","                     objective='multi:softprob',\n","                    # tree_method='gpu_hist', gpu_id=None,\n","                    max_depth=3,n_estimators=100,\n","                    min_child_weight=5,gamma=1,\n","                    subsample=0.8,learning_rate=1.0,\n","                    nthread=8,colsample_bytree=1.0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m4IRFiqEOFeJ"},"source":["# TODO: fill in the rest and report accuracy\n","clf.fit(train_hop3_feats, y_train)\n","\n","print(\"Module 3 took {0} seconds.\".format(time.time() - module3_start_time))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QdtdS35yOFjB"},"source":["y_train_pred = clf.predict(train_hop3_feats)\n","train_predictions = [round(value) for value in y_train_pred]\n","# evaluate accuracy for test \n","tr_acc = accuracy_score(y_train, train_predictions)\n","print(\"Train Accuracy: %.2f%%\" % (tr_acc * 100.0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMtDO3w3OKJO"},"source":["y_test_pred = clf.predict(test_hop3_feats)\n","test_predictions = [round(value) for value in y_test_pred]\n","# evaluate accuracy for test \n","te_acc = accuracy_score(y_test, test_predictions)\n","print(\"Test Accuracy: %.2f%%\" % (te_acc * 100.0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0S0v3f6v83sY"},"source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import plot_confusion_matrix\n","import seaborn as sns\n","\n","cf_matrix = confusion_matrix(y_test, y_test_pred)\n","print(cf_matrix)\n","  \n","# heatmap here and see percentages of data represented in each quadrant\n","plt.subplots(figsize =(15,8))\n","sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt='.2%',cmap='Blues',linewidths=.6, xticklabels='auto')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4sZWalQGOKP3"},"source":["\n","\n","##### with batched \n","\n","\n","objective='multi:softprob',\n","                    # tree_method='gpu_hist', gpu_id=None,\n","                    max_depth=3,n_estimators=100,\n","                    min_child_weight=5,gamma=1,\n","                    subsample=0.8,learning_rate=1.0,\n","                    nthread=8,colsample_bytree=1.0)\n","\n","## 1\n","TH2 = 0.001\n","TH1 = 0.005\n","l_r = 1.0\n","\n","Hop3 train features shape:  (60000, 9)\n","Hop3 test features shape:  (10000, 9)\n","\n","Hop1 Energy Shape : (23,)\n","Hop2 Energy Shape : (23,)\n","Hop3 Energy Shape : (23,)\n","\n","Module 1 took 13.90117073059082 seconds.\n","Module 2 took 42.53356671333313 seconds.\n","Module 3 took 38.52722501754761 seconds.\n","\n","Train Accuracy: 85.34%\n","Test Accuracy: 77.06%\n","\n","## 2\n","TH2 = 0.001\n","TH1 = 0.25\n","l_r = 1.0\n","\n","Hop3 train features shape:  (60000, 9)\n","Hop3 test features shape:  (10000, 9)\n","\n","\n","Hop1 Energy Shape : (23,)\n","Hop2 Energy Shape : (23,)\n","Hop3 Energy Shape : (23,)\n","\n","Module 1 took 13.694656133651733 seconds.\n","Module 2 took 42.759296894073486 seconds.\n","Module 3 took 64.41364765167236 seconds.\n","\n","Train Accuracy: 85.34%\n","Test Accuracy: 77.06%\n","\n","## 3\n","TH2 = 0.001\n","TH1 = 0.01\n","l_r = 1.0\n","\n","Hop3 train features shape:  (60000, 9)\n","Hop3 test features shape:  (10000, 9)\n","\n","Hop1 Energy Shape : (23,)\n","Hop2 Energy Shape : (23,)\n","Hop3 Energy Shape : (23,)\n","\n","Module 1 took 14.366409540176392 seconds.\n","Module 2 took 44.701276540756226 seconds.\n","Module 3 took 39.44400668144226 seconds.\n","\n","Train Accuracy: 85.34%\n","Test Accuracy: 77.06%\n","\n","## 4\n","TH2 = 0.001\n","TH1 = 0.00001\n","l_r = 1.0\n","\n","Hop1 Energy Shape : (23,)\n","Hop2 Energy Shape : (23,)\n","Hop3 Energy Shape : (23,)\n","\n","Hop3 train features shape:  (60000, 9)\n","Hop3 test features shape:  (10000, 9)\n","\n","\n","Module 1 took 14.366409540176392 seconds.\n","Module 2 took 44.542786836624146 seconds.\n","Module 3 took 40.646803855895996 seconds.\n","\n","Train Accuracy: 85.34%\n","Test Accuracy: 77.06%\n","\n","\n","## 5\n","TH2 = 0.001\n","TH1 = 0.0005\n","l_r = 1.0\n","\n","Hop3 train features shape:  (60000, 9)\n","Hop3 test features shape:  (10000, 9)\n","\n","Module 1 took 18.425812482833862 seconds.\n","Module 2 took 42.14961504936218 seconds.\n","Module 3 took 39.322232723236084 seconds.\n","\n","Train Accuracy: 85.34%\n","Test Accuracy: 77.06%\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7vNePrmbNUbc"},"source":[""],"execution_count":null,"outputs":[]}]}